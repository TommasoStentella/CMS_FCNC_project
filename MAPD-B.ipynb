{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52ef9c88",
   "metadata": {},
   "source": [
    "<h1 align='center'>Final assignment of 'Management and Analysis of Physics Datasets'</h1>\n",
    "\n",
    "<h2 align='center'>Part 1: Distributed computing: Spark</h2>\n",
    "\n",
    "<h3 align='left'>University of Padua - Physics of Data</h3>\n",
    "\n",
    "<h4 align='right'>Prof. Jacopo Pazzini</h4>\n",
    "<img align='center' src='https://www.unidformazione.com/wp-content/uploads/2018/04/unipd-universita-di-padova.png' alt='Drawing' style='width:400px;'/>\n",
    "\n",
    "<!-- <h5 align='center'>Group members</h5> -->\n",
    "\n",
    "**Name** | **ID number** | **mail**@studenti.unipd.it\n",
    ":-:|:-:|-:\n",
    "Chiara Maccani | 2027591 | chiara.maccani\n",
    "Samuele Piccinelli | 2027650 | samuele.piccinelli\n",
    "Tommaso Stentella | 2027586 | tommaso.stentella\n",
    "Cristina Venturini | 2022461 | cristina.venturini.5\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc89aef5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# - import libraries and useful dependencies\n",
    "\n",
    "import numpy as np\n",
    "import math as mh\n",
    "import uproot\n",
    "import requests\n",
    "import ssl\n",
    "\n",
    "import findspark\n",
    "findspark.init('/usr/local/spark')\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521bdf23",
   "metadata": {},
   "source": [
    "## 1. Setting up Spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c17d8d",
   "metadata": {},
   "source": [
    "The number of cores assigned to each executor is configurable. When `spark.executor.cores` is explicitly set, multiple executors from the same application may be launched on the same worker if the worker has enough cores and memory. Otherwise, each executor grabs all the cores available on the worker by default, in which case only one executor per application may be launched on each worker during one single schedule iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce439cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.nio.file.FileSystemException: /root/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.9.9.jar -> /tmp/spark-068be7b0-e00f-47ff-8006-64f1808ffc4f/userFiles-b1dd9d6b-8e76-4b61-9310-987629024498/com.fasterxml.jackson.core_jackson-databind-2.9.9.jar: No space left on device\n\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n\tat sun.nio.fs.UnixCopyFile.copyFile(UnixCopyFile.java:253)\n\tat sun.nio.fs.UnixCopyFile.copy(UnixCopyFile.java:581)\n\tat sun.nio.fs.UnixFileSystemProvider.copy(UnixFileSystemProvider.java:253)\n\tat java.nio.file.Files.copy(Files.java:1274)\n\tat org.apache.spark.util.Utils$.copyRecursive(Utils.scala:726)\n\tat org.apache.spark.util.Utils$.copyFile(Utils.scala:697)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:771)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:541)\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1627)\n\tat org.apache.spark.SparkContext.$anonfun$new$13(SparkContext.scala:508)\n\tat org.apache.spark.SparkContext.$anonfun$new$13$adapted(SparkContext.scala:508)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:508)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f77e5806c531>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.jars.packages'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'org.apache.hadoop:hadoop-aws:2.7.3,ch.cern.sparkmeasure:spark-measure_2.12:0.17'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.executor.cores'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.executor.memory'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'6g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 147\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \"\"\"\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1569\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.nio.file.FileSystemException: /root/.ivy2/jars/com.fasterxml.jackson.core_jackson-databind-2.9.9.jar -> /tmp/spark-068be7b0-e00f-47ff-8006-64f1808ffc4f/userFiles-b1dd9d6b-8e76-4b61-9310-987629024498/com.fasterxml.jackson.core_jackson-databind-2.9.9.jar: No space left on device\n\tat sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)\n\tat sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n\tat sun.nio.fs.UnixCopyFile.copyFile(UnixCopyFile.java:253)\n\tat sun.nio.fs.UnixCopyFile.copy(UnixCopyFile.java:581)\n\tat sun.nio.fs.UnixFileSystemProvider.copy(UnixFileSystemProvider.java:253)\n\tat java.nio.file.Files.copy(Files.java:1274)\n\tat org.apache.spark.util.Utils$.copyRecursive(Utils.scala:726)\n\tat org.apache.spark.util.Utils$.copyFile(Utils.scala:697)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:771)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:541)\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1627)\n\tat org.apache.spark.SparkContext.$anonfun$new$13(SparkContext.scala:508)\n\tat org.apache.spark.SparkContext.$anonfun$new$13$adapted(SparkContext.scala:508)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:508)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master('spark://10.67.22.59:7077') \\\n",
    "        .appName('first') \\\n",
    "        .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:2.7.3,ch.cern.sparkmeasure:spark-measure_2.12:0.17') \\\n",
    "        .config('spark.executor.cores', '2') \\\n",
    "        .config('spark.executor.memory','6g') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81b33dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d32baf02",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ed5a16d870c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afacfd96",
   "metadata": {},
   "source": [
    "## 2. Read `.root` files stored remotely from S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c686a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.expanduser('~/.s3cfg'))\n",
    "access_id = config.get('aws_profile', 'access_key') \n",
    "access_key = config.get('aws_profile', 'secret_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e1e8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_conf=sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set('fs.s3n.impl', 'org.apache.hadoop.fs.s3native.NativeS3FileSystem')\n",
    "hadoop_conf.set('fs.s3n.awsAccessKeyId', access_id)\n",
    "hadoop_conf.set('fs.s3n.awsSecretAccessKey', access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd6b4e8",
   "metadata": {},
   "source": [
    "## 3. Path and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "buck = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/LCPmodB-Y3_CMS_FCNC/'\n",
    "\n",
    "listDir = requests.get(buck, verify=False).text.split('\\n')\n",
    "# listDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519c32b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnDir(string):\n",
    "    return [filename for filename in listDir if filename.startswith(string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3639910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Data\n",
    "dataDirs = [buck + file for file in returnDir('SingleMuon')]\n",
    "\n",
    "# - MC Signal\n",
    "signalMCDirs = [buck + file for file in returnDir('TT_FCNC')]\n",
    "\n",
    "# - MC backgrounds\n",
    "listBkgDir = ['ST_', 'TTTT_Tune', 'TTTo2L2Nu', 'TTToHadronic', 'TTToSemiLeptonic', 'TTWJetsToLNu', 'TTZToLLNuNu',\n",
    "              'WGToLNuG', 'WJetsToLNu', 'WWTo2L2Nu', 'WWW', 'WWZ', 'WZG', 'WZTo1L1Nu2Q', 'WZTo2L2Q', 'WZTo3LNu',\n",
    "              'WZZ', 'WmWmJJ', 'WminusH', 'WpWpJJ', 'ZG', 'ZZ', 'tZq', 'DYJetsToLL_M', 'QCD']\n",
    "\n",
    "bkgMCDirs = dict(list(zip(listBkgDir, map(returnDir, listBkgDir))))\n",
    "\n",
    "for j in listBkgDir:\n",
    "    for i in range(len(bkgMCDirs[j])):\n",
    "        bkgMCDirs[j][i] = buck + bkgMCDirs[j][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3902e08f",
   "metadata": {},
   "source": [
    "## 4. Read and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf7e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - custom data loader\n",
    "def readRoot(file_name):\n",
    "    \n",
    "    if (not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None)):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    \n",
    "    var = ['iSkim', 'event', 'eventWeightLumi', 'nMuon','Muon_pfRelIso03_all', 'Muon_pt', 'Muon_eta',\n",
    "               'Muon_phi', 'Muon_charge', 'nElectron','Electron_pfRelIso03_all','Electron_pt', 'Electron_phi',\n",
    "               'Electron_charge','nCleanJet','nBJet','MET_pt', 'MET_phi']\n",
    "\n",
    "    pro = uproot.open(file_name + ':Events').arrays(var, library='np')\n",
    "    nEvent = len(pro['event'])\n",
    "    custom_data = [0 for _ in range(nEvent)]\n",
    "\n",
    "    for i in range(nEvent):\n",
    "        event = dict(list(zip(var, [pro[v][i].tolist() for v in var])))\n",
    "        custom_data[i] = event\n",
    "        \n",
    "    return custom_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554b4cf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# - load data\n",
    "dfData = sc.parallelize(dataDirs).flatMap(readRoot).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f65ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - load MC backgrounds\n",
    "dfMCBkg = {}\n",
    "for key, value in bkgMCDirs.items():\n",
    "    dfMCBkg[key] = sc.parallelize(value).flatMap(readRoot).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6150c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - load MC signal\n",
    "dfMCSig = sc.parallelize(signalMCDirs).flatMap(readRoot).toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df317a",
   "metadata": {},
   "source": [
    "## Measuring performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a5fc68",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'returnDir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-183ab394ff97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# flat_list = [item for sublist in dicto for item in sublist]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# returnDir('tZq')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlisto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'/data/FCNC/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiro\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdiro\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreturnDir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tZq'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlisto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1e9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'returnDir' is not defined"
     ]
    }
   ],
   "source": [
    "# dicto =  list(map(returnDir, listBkgDir))\n",
    "# flat_list = [item for sublist in dicto for item in sublist]\n",
    "# returnDir('tZq')\n",
    "listo = ['/data/FCNC/' + diro for diro in returnDir('tZq')]\n",
    "\n",
    "size = sum(os.path.getsize(f) for f in listo)/1e9\n",
    "size\n",
    "size#*1e3/56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b0027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_dir = bkgMCDirs['tZq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f709c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkmeasure import StageMetrics\n",
    "stagemetrics = StageMetrics(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116c1e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import (register_line_magic, register_cell_magic, register_line_cell_magic)\n",
    "\n",
    "#@register_line_cell_magic\n",
    "def sparkmeasure(df, idx, ex = 2):#line, cell=None):\n",
    "    #val = cell if cell is not None else line\n",
    "    stagemetrics.begin()\n",
    "    #eval(val)\n",
    "    df.count()\n",
    "    stagemetrics.end()\n",
    "    \n",
    "    df_out = stagemetrics.create_stagemetrics_DF(\"PerfStageMetrics\")\n",
    "    stagemetrics.save_data(df_out, \"./perf_test/stagemetrics_{}_{}\".format(ex, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "part = [4,12,24,36,48,56,64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6189c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "trialdf = [0,0,0,0,0,0,0]\n",
    "for i in range(len(part)): \n",
    "    trialdf[i] = sc.parallelize(trial_dir, part[i]).flatMap(readRoot).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a76f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparkmeasure(trialdf, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkmeasure(trialdf[0], part[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e9c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkmeasure(trialdf[1], part[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad9647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkmeasure(trialdf[2], part[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da0d8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkmeasure(trialdf[3], part[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkmeasure(trialdf[4], part[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b292924",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkmeasure(trialdf[5], part[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef5ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkmeasure(trialdf[6], part[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825b045",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%sparkmeasure\n",
    "trialdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca24d8",
   "metadata": {},
   "source": [
    "## 5. Skimming and HLF creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e0c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfData.count()\n",
    "# dfData.printSchema()\n",
    "# dfData.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05291c41",
   "metadata": {},
   "source": [
    "### 5.1 First level filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0581b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as pf\n",
    "import pyspark.sql.types as pt\n",
    "\n",
    "arr = pt.ArrayType\n",
    "maskMu = pf.udf(lambda iso, pt: [True if i < .15 and p > 15 else False for i,p in zip(iso,pt)], arr(pt.BooleanType()))\n",
    "maskEl = pf.udf(lambda iso: [True if i < .15 else False for i in iso], arr(pt.BooleanType()))\n",
    "apply_mask_long = pf.udf(lambda x, mask: [i for i,m in zip(x,mask) if m], arr(pt.LongType()))\n",
    "apply_mask_doub = pf.udf(lambda x, mask: [i for i,m in zip(x,mask) if m], arr(pt.DoubleType()))\n",
    "count_col = pf.udf(lambda m: abs(sum(m)), pt.IntegerType())\n",
    "mass_mu = pf.lit(.1057)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5bb0b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dfSkimmed(df):\n",
    "    fdf = df.filter('iSkim == 3')\\\n",
    "    .withColumn('maskMu', maskMu('Muon_pfRelIso03_all', 'Muon_pt'))\\\n",
    "    .withColumn('nGoodMu', count_col('maskMu'))\\\n",
    "    .withColumn('maskEl', maskEl('Electron_pfRelIso03_all'))\\\n",
    "    .drop('Muon_pfRelIso03_all', 'Electron_pfRelIso03_all')\\\n",
    "    .withColumn('nGoodEl', count_col('maskEl'))\\\n",
    "    .filter('(nGoodMu == 3) AND (nGoodEl == 0)')\\\n",
    "    .drop('nGoodMu', 'nGoodEl')\\\n",
    "    .withColumn('SumCharge', count_col(apply_mask_long('Muon_charge', 'maskMu')))\\\n",
    "    .filter('SumCharge != 3')\\\n",
    "    .drop('SumCharge')\\\n",
    "    .withColumn('sMuon_pt', apply_mask_doub('Muon_pt', 'maskMu'))\\\n",
    "    .withColumn('sMuon_eta', apply_mask_doub('Muon_eta', 'maskMu'))\\\n",
    "    .withColumn('sMuon_phi', apply_mask_doub('Muon_phi', 'maskMu'))\\\n",
    "    .withColumn('sMuon_charge', apply_mask_long('Muon_charge', 'maskMu'))\\\n",
    "    .withColumn('sMuon_mass', pf.array(mass_mu, mass_mu, mass_mu))\\\n",
    "    .drop('Muon_pt', 'Muon_eta', 'Muon_phi', 'Muon_charge')\\\n",
    "    .filter('(nCleanJet >= 2) AND (nBJet >= 1)')\\\n",
    "    .drop('nCleanJet', 'nBJet')\n",
    "    \n",
    "    return fdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa40bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata = dfSkimmed(dfData)\n",
    "fsignal = dfSkimmed(dfMCSig)\n",
    "\n",
    "fbkg = {}\n",
    "for key, value in dfMCBkg.items():\n",
    "    fbkg[key] = dfSkimmed(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ac0c5",
   "metadata": {},
   "source": [
    "### 5.2 Second level filter: $\\Delta R$, $\\Delta\\phi$ and $m_{inv}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a3a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeltaR(eta1, eta2, phi1, phi2):\n",
    "    return mh.sqrt((eta1-eta2)**2 + (phi1-phi2)**2)\n",
    "\n",
    "@pf.udf(arr(pt.IntegerType()))\n",
    "def find_idx(charge, phi, eta):\n",
    "   \n",
    "    idx = [-1]*len(charge)\n",
    "    \n",
    "    if(charge[0]==charge[1] and charge[1]!=charge[2]):\n",
    "        idx[0] = 2\n",
    "        dr_1 = DeltaR(eta[1], eta[2], phi[1], phi[2])\n",
    "        dr_2 = DeltaR(eta[0], eta[2], phi[0], phi[2])\n",
    "        \n",
    "        if(dr_1 < dr_2): idx[1], idx[2] = 1, 0\n",
    "        else: idx[1], idx[2] = 0, 1\n",
    "        \n",
    "    if(charge[0]!=charge[1] and charge[1]!=charge[2]): \n",
    "        idx[0] = 1\n",
    "        dr_1 = DeltaR(eta[1], eta[2], phi[1], phi[2])\n",
    "        dr_2 = DeltaR(eta[0], eta[1], phi[0], phi[1])\n",
    "        \n",
    "        if(dr_1 < dr_2): idx[1], idx[2] = 2, 0\n",
    "        else: idx[1], idx[2] = 2, 0\n",
    "    \n",
    "    if(charge[0]!=charge[1] and charge[1]==charge[2]):\n",
    "        idx[0] = 0\n",
    "        dr_1 = DeltaR(eta[0], eta[2], phi[0], phi[2])\n",
    "        dr_2 = DeltaR(eta[0], eta[1], phi[0], phi[1])\n",
    "        \n",
    "        if(dr_1 < dr_2): idx[1], idx[2] = 2, 1\n",
    "        else: idx[1], idx[2] = 1, 2   \n",
    "        \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4065a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dR = pf.udf(lambda eta1, eta2, phi1, phi2: DeltaR(eta1, eta2, phi1, phi2), pt.DoubleType())\n",
    "\n",
    "return_value_long = pf.udf(lambda x,idx,val: x[idx[val]], pt.LongType())\n",
    "return_value_doub = pf.udf(lambda x,idx,val: x[idx[val]], pt.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3774c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invMass(pt, eta, phi, mass):\n",
    "    \n",
    "    size = len(pt)\n",
    "    assert(len(eta) == size and len(phi) == size and len(mass) == size)\n",
    "  \n",
    "    x_sum, y_sum, z_sum, e_sum = 0., 0., 0., 0.\n",
    "  \n",
    "    for i in range(size):\n",
    "        # Convert to (e, x, y, z) coordinate system and update sums\n",
    "        x = pt[i] * mh.cos(phi[i])\n",
    "        x_sum += x\n",
    "        y = pt[i] * mh.sin(phi[i])\n",
    "        y_sum += y\n",
    "        z = pt[i] * mh.sinh(eta[i])\n",
    "        z_sum += z\n",
    "        e = mh.sqrt(x * x + y * y + z * z + mass[i] * mass[i])\n",
    "        e_sum += e\n",
    "        \n",
    "    return mh.sqrt(e_sum * e_sum - x_sum * x_sum - y_sum * y_sum - z_sum * z_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce367944",
   "metadata": {},
   "outputs": [],
   "source": [
    "invMass3 = pf.udf(lambda (pt, eta, phi, mass): invMass(pt, eta, phi, mass), pt.DoubleType())\n",
    "invMass2 = pf.udf(lambda (pt1, pt2, eta1, eta2, phi1, phi2, mass1, mass2):\n",
    "                 invMass([pt1, pt2], [eta1, eta2], [phi1, phi2], [mass1, mass2]), pt.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625d8e90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dfHLF(df):\n",
    "    fdf = df.withColumn('mu_idx', find_idx('sMuon_charge', 'sMuon_phi', 'sMuon_eta'))\\\n",
    "    .withColumn('mu_pt0', return_value_doub('sMuon_pt', 'mu_idx', pf.lit(0)))\\\n",
    "    .withColumn('mu_pt1', return_value_doub('sMuon_pt', 'mu_idx', pf.lit(1)))\\\n",
    "    .withColumn('mu_pt2', return_value_doub('sMuon_pt', 'mu_idx', pf.lit(2)))\\\n",
    "    .withColumn('mu_eta0', return_value_doub('sMuon_eta', 'mu_idx', pf.lit(0)))\\\n",
    "    .withColumn('mu_eta1', return_value_doub('sMuon_eta', 'mu_idx', pf.lit(1)))\\\n",
    "    .withColumn('mu_eta2', return_value_doub('sMuon_eta', 'mu_idx', pf.lit(2)))\\\n",
    "    .withColumn('mu_phi0', return_value_doub('sMuon_phi', 'mu_idx', pf.lit(0)))\\\n",
    "    .withColumn('mu_phi1', return_value_doub('sMuon_phi', 'mu_idx', pf.lit(1)))\\\n",
    "    .withColumn('mu_phi2', return_value_doub('sMuon_phi', 'mu_idx', pf.lit(2)))\\\n",
    "    .withColumn('mu_q0', return_value_long('sMuon_charge', 'mu_idx', pf.lit(0)))\\\n",
    "    .withColumn('mu_q1', return_value_long('sMuon_charge', 'mu_idx', pf.lit(1)))\\\n",
    "    .withColumn('mu_q2', return_value_long('sMuon_charge', 'mu_idx', pf.lit(2)))\\\n",
    "    .withColumn('mu_mass0', pf.lit(0.1057))\\\n",
    "    .withColumn('mu_mass1', pf.lit(0.1057))\\\n",
    "    .withColumn('mu_mass2', pf.lit(0.1057))\\\n",
    "    .withColumn('inv_m3', invMass3('sMuon_pt', 'sMuon_eta', 'sMuon_phi', 'sMuon_mass'))\\\n",
    "    .withColumn('inv_m01', invMass2('mu_pt0', 'mu_pt1', 'mu_eta0', 'mu_eta1', 'mu_phi0', 'mu_phi1', 'mu_mass0', 'mu_mass1'))\\\n",
    "    .withColumn('inv_m02', invMass2('mu_pt0', 'mu_pt2', 'mu_eta0', 'mu_eta2', 'mu_phi0', 'mu_phi2', 'mu_mass0', 'mu_mass2'))\\\n",
    "    .withColumn('inv_m12', invMass2('mu_pt1', 'mu_pt2', 'mu_eta1', 'mu_eta2', 'mu_phi1', 'mu_phi2', 'mu_mass1', 'mu_mass2'))\\\n",
    "    .filter('(inv_m01 > 15) AND (inv_m02 > 15) AND (inv_m12 > 15)')\\\n",
    "    .filter('(abs(inv_m01 - 91.2) > 10) AND (abs(inv_m02 - 91.2) > 10)')\\\n",
    "    .withColumn('dR01', dR('mu_eta0', 'mu_eta1', 'mu_phi0', 'mu_phi1'))\\\n",
    "    .withColumn('dR02', dR('mu_eta0', 'mu_eta2', 'mu_phi0', 'mu_phi2'))\\\n",
    "    .withColumn('dPhi0', pf.abs(pf.col('MET_phi') - pf.col('mu_phi0')))\\\n",
    "    .withColumn('dPhi1', pf.abs(pf.col('MET_phi') - pf.col('mu_phi1')))\\\n",
    "    .withColumn('dPhi2', pf.abs(pf.col('MET_phi') - pf.col('mu_phi2')))\n",
    "    \n",
    "    return fdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e07ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dfHLF(fdata)\n",
    "signal = dfHLF(fsignal)\n",
    "\n",
    "bkg = {}\n",
    "for key, value in fbkg.items():\n",
    "    bkg[key] = dfHLF(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5edd91",
   "metadata": {},
   "source": [
    "## 6. Save the datasets as parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ee3c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_PATH = \"./dataMAPD\"\n",
    "%time features.write.parquet(DATASETS_PATH+\"datasets.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460afd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read \\\n",
    "            .format(\"parquet\") \\\n",
    "            .load(\"hdfs://hadalytic/project/ML/data/swan/datasets.parquet\")\n",
    "\n",
    "events = data.count()\n",
    "print(\"There are {} events\".format(events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6e0c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2c5aae4",
   "metadata": {},
   "source": [
    "## 7. Final histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ffba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histo(variable, weight, range_, bins):\n",
    "    edges = np.array([i*(range_[1]-range_[0])/bins for i in range(0,bins+1)])\n",
    "    edges = edges+range_[0]\n",
    "    \n",
    "    lim_couples = [[edges[i], edges[i+1]] for i in range(len(edges)-1)]\n",
    "    \n",
    "    def filtr(limits):\n",
    "        if type(variable) != list:\n",
    "            if (variable >= limits[0]) and (variable < limits[1]): return 1\n",
    "            else: return 0\n",
    "        else:\n",
    "            lim0 = np.full(len(variable),limits[0])\n",
    "            lim1 = np.full(len(variable),limits[1])\n",
    "            mask = np.logical_and((variable >= lim0), (variable < lim1))\n",
    "            if np.any(mask): return np.sum(mask)\n",
    "            else: return 0\n",
    "    \n",
    "    counts = np.array(list(map(filtr, lim_couples))) * weight\n",
    "    \n",
    "    return edges, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ea6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(variable, interval, nbins):\n",
    "    \n",
    "    ### HISTOGRAM BINS COUNTING ###\n",
    "    \n",
    "    data_bins, data_counts = (\n",
    "            data.select([variable,'eventWeightLumi']).rdd\n",
    "                .map(lambda x: histo(x[variable], x['eventWeightLumi'], range_=interval, bins=nbins))\n",
    "                .reduce(lambda x,y: (x[0], x[1]+y[1])))\n",
    "    data_bin_centers = data_bins[:-1] + np.diff(data_bins)/2                                                                                   \n",
    "\n",
    "    SigMC_bins, SigMC_counts = (\n",
    "            signal.select([variable,'eventWeightLumi']).rdd\n",
    "                  .map(lambda x: histo(x[variable], x['eventWeightLumi'], range_=interval, bins=nbins))\n",
    "                  .reduce(lambda x,y: (x[0], x[1]+y[1])))\n",
    "    SigMC_bin_centers = SigMC_bins[:-1] + np.diff(SigMC_bins)/2  \n",
    "\n",
    "    #MCbkg_hist_results = {}\n",
    "    bkgMC_bin_centers = []\n",
    "    bkgMC_counts = []\n",
    "    bkgMC_labels = dfMCBkg.keys()\n",
    "\n",
    "    for value in bkg.values():\n",
    "        #MCbkg_hist_results[key] = {}\n",
    "        bkg_rdd = value.select([variable,'eventWeightLumi']).rdd\n",
    "        if bkg_rdd.isEmpty():\n",
    "            counts = np.full(nbins, 0)\n",
    "            bins = np.array([i*(interval[1]-interval[0])/nbins for i in range(0, nbins+1)])\n",
    "            bins = bins + interval[0]\n",
    "        else:\n",
    "            bins, counts = (\n",
    "                bkg_rdd\n",
    "                    .map(lambda x: histo(x[variable], x['eventWeightLumi'], range_=interval, bins=nbins))\n",
    "                    .reduce(lambda x,y: (x[0], x[1]+y[1])))\n",
    "        bin_centers = bins[:-1] + np.diff(bins)/2\n",
    "\n",
    "        bkgMC_bin_centers.append(bin_centers)                                                                                             \n",
    "        bkgMC_counts.append(counts)\n",
    "        \n",
    "    ### SIGNAL RESCALING ###\n",
    "        \n",
    "    bkg_values, bkg_bins, _ = plt.hist(bkgMC_bin_centers, weights=bkgMC_counts, bins=bins,\n",
    "                                       stacked=True, histtype= 'barstacked')\n",
    "    areaBkg = sum(np.diff(bkg_bins)*bkg_values)\n",
    "    sig_values, sig_bins, _ = plt.hist(SigMC_bin_centers, weights=SigMC_counts, bins=SigMC_bins)\n",
    "    areaSig = sum(np.diff(sig_bins)*sig_values)\n",
    "    scale_MCSig = areaBkg/areaSig\n",
    "\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    ### BACKGROUND COLORS ###\n",
    "    repeat = [1,4,1,1,15,3]\n",
    "    colors = [(222/255, 90/255, 106/255),(248/255, 206/255, 104/255),(155/255, 152/255, 204/255),\n",
    "                (247/255, 138/255, 221/255),(250/255, 202/255, 255/255),(100/255, 192/255, 232/255)]\n",
    "    bkg_colors = [item for item, count in zip(colors, repeat) for i in range(count)]\n",
    "    \n",
    "    \n",
    "    ### PLOTTING ###\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "    plt.hist(bkgMC_bin_centers, weights=bkgMC_counts, bins=bins, label=bkgMC_labels,\n",
    "             alpha=1., color=bkg_colors, stacked=True, histtype= 'barstacked')\n",
    "                                                                                                  \n",
    "    plt.errorbar(data_bin_centers, data_counts, yerr=np.sqrt(data_counts), \n",
    "                 fmt='o', ms=4, lw=1, color='black', label='Data', capsize=3)\n",
    "\n",
    "    plt.hist(SigMC_bin_centers, weights=SigMC_counts, bins=SigMC_bins, #*scale_MCSig\n",
    "             label='MCSig', color='#00A88F', alpha=1.,  histtype=u'step', linewidth=2)\n",
    "                                                                                                \n",
    "    plt.xlabel(variable)\n",
    "    plt.ylabel('counts')\n",
    "    \n",
    "    ST_patch = patches.Patch(color=bkg_colors[0], label='Single Top')\n",
    "    TT_patch = patches.Patch(color=bkg_colors[1], label='Top Top')\n",
    "    TTW_patch = patches.Patch(color=bkg_colors[5], label=r'tt$\\rightarrow$ W')\n",
    "    TTZ_patch = patches.Patch(color=bkg_colors[6], label=r'tt$\\rightarrow$ Z')\n",
    "    Diboson_patch = patches.Patch(color=bkg_colors[7], label='Diboson')\n",
    "    Others_patch = patches.Patch(color=bkg_colors[-1], label='Others')\n",
    "    MCSig_patch = patches.Patch(color='#00A88F', label='MCSig')\n",
    "    Data = Line2D([0], [0], marker='o', color='w', label='Data', markerfacecolor='black', markersize=7)\n",
    "\n",
    "    plt.legend(handles=[ST_patch,TT_patch, TTW_patch, TTZ_patch, Diboson_patch, Others_patch, MCSig_patch, Data])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92148eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram('inv_m01', (0,200), 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215eb2e1",
   "metadata": {},
   "source": [
    "## OTHERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a4484d",
   "metadata": {},
   "source": [
    "#### Trials pyRDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c7606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('appName').setMaster('spark://10.67.22.59:7077')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94941842",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_raw = requests.get(buck, verify=False)\n",
    "file_list = file_list_raw.text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(file_list)):\n",
    "    file_list[i] = 's3' + buck + file_list[i]\n",
    "    \n",
    "chain = ROOT.TChain('Events')\n",
    "for file in file_list[:2]:\n",
    "    chain.AddFile(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e629166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = PyRDF.RDataFrame(chain, sparkcontext = sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc3d14",
   "metadata": {},
   "source": [
    "#### Distributed PyROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(file_list)):\n",
    "    file_list[i] = 's3' + buck + file_list[i]\n",
    "    \n",
    "chain = ROOT.TChain('Events')\n",
    "for file in file_list[:1]:\n",
    "    chain.AddFile(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969021ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point RDataFrame calls to the Spark specific RDataFrame\n",
    "RDataFrame = ROOT.RDF.Experimental.Distributed.Spark.RDataFrame\n",
    "\n",
    "df = RDataFrame(chain, sparkcontext = sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2738f82e",
   "metadata": {},
   "source": [
    "#### Stop cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5504b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2602ca",
   "metadata": {},
   "source": [
    "### To do:\n",
    "\n",
    "- risorse spark app [here](https://spark.apache.org/docs/latest/configuration.html#memory-management)\n",
    "- partitions prima o dopo il loading file\n",
    "- maybe insert: interesting statistics, comparison with root loaders\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
