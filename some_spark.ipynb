{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc89aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "import requests\n",
    "import findspark\n",
    "findspark.init('/usr/local/spark')\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eacc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some of the env variables\n",
    "!env | grep -i spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd783084",
   "metadata": {},
   "outputs": [],
   "source": [
    "buck = 'https://cloud-areapd.pd.infn.it:5210/swift/v1/AUTH_d2e941ce4b324467b6b3d467a923a9bc/LCPmodB-Y3_CMS_FCNC/'\n",
    "\n",
    "local = '/data/FCNC/'\n",
    "\n",
    "tofile = 'SingleMuon_Run2016B_ver1-Nano1June2019_ver1-v1/4F32E8E2-3E72-E84D-A12B-CA4CB942D65C_Skim.root'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521bdf23",
   "metadata": {},
   "source": [
    "#### Trials with ROOT documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c87733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import boto3\n",
    "\n",
    "#s3 = boto3.resource('s3')\n",
    "#bucket = s3.Bucket(buck)\n",
    "#cl = boto3.client('s3')\n",
    "#response = cl.list_objects(Bucket=buck)\n",
    "\n",
    "#bucket.objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce439cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a SparkContext object with the right configuration for your Spark cluster\n",
    "conf = SparkConf().setAppName('appName').setMaster('spark://10.67.22.59:7077')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_raw = requests.get(buck, verify=False)\n",
    "file_list = file_list_raw.text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(file_list)):\n",
    "    file_list[i] = 's3' + buck + file_list[i]\n",
    "    \n",
    "chain = ROOT.TChain('Events')\n",
    "for file in file_list:\n",
    "    chain.AddFile(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969021ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point RDataFrame calls to the Spark specific RDataFrame\n",
    "RDataFrame = ROOT.RDF.Experimental.Distributed.Spark.RDataFrame\n",
    " \n",
    "# The Spark RDataFrame constructor accepts an optional \"sparkcontext\" parameter\n",
    "# and it will distribute the application to the connected cluster\n",
    "df = RDataFrame(chain, sparkcontext = sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb10d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.Filter('nMuon > 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed6c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Count().GetValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4a08dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ROOT.RDataFrame('Events', file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b109cfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe33ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why doesnt it work?\n",
    "sums = df.Filter(\"nMuon > 2\").Sum(\"nElectron\")\n",
    "h = df.Histo1D(\"nMuon\")\n",
    " \n",
    "print(sums.GetValue())\n",
    "h.Draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcddb531",
   "metadata": {},
   "source": [
    "#### Trials with DIANA-HEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd9547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49792dc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master('spark://10.67.22.59:7077') \\\n",
    "        .appName('3rd') \\\n",
    "        .config('spark.jars.packages',\n",
    "                'org.diana-hep:spark-root_2.11:0.1.13,org.diana-hep:histogrammar-sparksql_2.11:1.0.4,org.apache.hadoop:hadoop-aws:2.7.0',)\\\n",
    "        .config('spark.cores.max',3)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791cdaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get spark context -> entry point used to work with RDD\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "df = spark.read.format('org.dianahep.sparkroot').load('hdfs:///root' + tofile)\n",
    "\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c7fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "sqlContext.read.format('org.dianahep.sparkroot').option('tree', 'Events')\\\n",
    ".load('file:'+tofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103181f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!$SPARK_HOME/bin/pyspark --master spark://ip-10.67.22.59:7077 --packages org.apache.hadoop:hadoop-aws:2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96703cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the ROOT file into a Spark DataFrame\n",
    "df = spark.read\\\n",
    "    .format('org.dianahep.sparkroot')\\\n",
    "    .load('s3://' + buck + tofile)\n",
    "# ... and print the number of events\n",
    "# print df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75eab48",
   "metadata": {},
   "source": [
    "#### Verifying spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53bb2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python dataset\n",
    "data = [1,2,3,4,5,6,7,8]\n",
    "\n",
    "# parallelize\n",
    "sc.parallelize(data).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dceb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def inside(p):     \n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "num_samples = 1000000\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4.0 * count / num_samples\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2738f82e",
   "metadata": {},
   "source": [
    "#### Stop cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5504b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
