// Class: ReadBDT2_20_Grad
// Automatically generated by MethodBase::MakeClass
//

/* configuration options =====================================================

#GEN -*-*-*-*-*-*-*-*-*-*-*- general info -*-*-*-*-*-*-*-*-*-*-*-

Method         : BDT::BDT2_20_Grad
TMVA Release   : 4.2.1         [262657]
ROOT Release   : 6.24/00       [399360]
Creator        : root
Date           : Sun Jul 18 20:03:26 2021
Host           : Linux 3bfcc9c0a42a 4.15.0-1111-azure #123~16.04.1-Ubuntu SMP Sat Mar 20 01:52:07 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
Dir            : /root
Training events: 137141
Analysis type  : [Classification]


#OPT -*-*-*-*-*-*-*-*-*-*-*-*- options -*-*-*-*-*-*-*-*-*-*-*-*-

# Set by User:
V: "False" [Verbose output (short form of "VerbosityLevel" below - overrides the latter one)]
NTrees: "20" [Number of trees in the forest]
MaxDepth: "3" [Max depth of the decision tree allowed]
MinNodeSize: "2.5%" [Minimum percentage of training events required in a leaf node (default: Classification: 5%, Regression: 0.2%)]
BoostType: "Grad" [Boosting type for the trees in the forest (note: AdaCost is still experimental)]
SeparationType: "sdivsqrtsplusb" [Separation criterion for node splitting]
# Default:
VerbosityLevel: "Default" [Verbosity level]
VarTransform: "None" [List of variable transformations performed before training, e.g., "D_Background,P_Signal,G,N_AllClasses" for: "Decorrelation, PCA-transformation, Gaussianisation, Normalisation, each for the given class of events ('AllClasses' denotes all events of all classes, if no class indication is given, 'All' is assumed)"]
H: "False" [Print method-specific help message]
CreateMVAPdfs: "False" [Create PDFs for classifier outputs (signal and background)]
IgnoreNegWeightsInTraining: "False" [Events with negative weights are ignored in the training (but are included for testing and performance evaluation)]
nCuts: "20" [Number of grid points in variable range used in finding optimal cut in node splitting]
AdaBoostR2Loss: "quadratic" [Type of Loss function in AdaBoostR2]
UseBaggedBoost: "False" [Use only a random subsample of all events for growing the trees in each boost iteration.]
Shrinkage: "1.000000e+00" [Learning rate for BoostType=Grad algorithm]
AdaBoostBeta: "5.000000e-01" [Learning rate  for AdaBoost algorithm]
UseRandomisedTrees: "False" [Determine at each node splitting the cut variable only as the best out of a random subset of variables (like in RandomForests)]
UseNvars: "3" [Size of the subset of variables used with RandomisedTree option]
UsePoissonNvars: "True" [Interpret "UseNvars" not as fixed number but as mean of a Poisson distribution in each split with RandomisedTree option]
BaggedSampleFraction: "6.000000e-01" [Relative size of bagged event sample to original size of the data sample (used whenever bagging is used (i.e. UseBaggedBoost, Bagging,)]
UseYesNoLeaf: "True" [Use Sig or Bkg categories, or the purity=S/(S+B) as classification of the leaf node -> Real-AdaBoost]
NegWeightTreatment: "pray" [How to treat events with negative weights in the BDT training (particular the boosting) : IgnoreInTraining;  Boost With inverse boostweight; Pair events with negative and positive weights in training sample and *annihilate* them (experimental!)]
Css: "1.000000e+00" [AdaCost: cost of true signal selected signal]
Cts_sb: "1.000000e+00" [AdaCost: cost of true signal selected bkg]
Ctb_ss: "1.000000e+00" [AdaCost: cost of true bkg    selected signal]
Cbb: "1.000000e+00" [AdaCost: cost of true bkg    selected bkg ]
NodePurityLimit: "5.000000e-01" [In boosting/pruning, nodes with purity > NodePurityLimit are signal; background otherwise.]
RegressionLossFunctionBDTG: "huber" [Loss function for BDTG regression.]
HuberQuantile: "7.000000e-01" [In the Huber loss function this is the quantile that separates the core from the tails in the residuals distribution.]
DoBoostMonitor: "False" [Create control plot with ROC integral vs tree number]
UseFisherCuts: "False" [Use multivariate splits using the Fisher criterion]
MinLinCorrForFisher: "8.000000e-01" [The minimum linear correlation between two variables demanded for use in Fisher criterion in node splitting]
UseExclusiveVars: "False" [Variables already used in fisher criterion are not anymore analysed individually for node splitting]
DoPreselection: "False" [and and apply automatic pre-selection for 100% efficient signal (bkg) cuts prior to training]
SigToBkgFraction: "1.000000e+00" [Sig to Bkg ratio used in Training (similar to NodePurityLimit, which cannot be used in real adaboost]
PruneMethod: "nopruning" [Note: for BDTs use small trees (e.g.MaxDepth=3) and NoPruning:  Pruning: Method used for pruning (removal) of statistically insignificant branches ]
PruneStrength: "0.000000e+00" [Pruning strength]
PruningValFraction: "5.000000e-01" [Fraction of events to use for optimizing automatic pruning.]
SkipNormalization: "False" [Skip normalization at initialization, to keep expectation value of BDT output according to the fraction of events]
nEventsMin: "0" [deprecated: Use MinNodeSize (in % of training events) instead]
UseBaggedGrad: "False" [deprecated: Use *UseBaggedBoost* instead:  Use only a random subsample of all events for growing the trees in each iteration.]
GradBaggingFraction: "6.000000e-01" [deprecated: Use *BaggedSampleFraction* instead: Defines the fraction of events to be used in each iteration, e.g. when UseBaggedGrad=kTRUE. ]
UseNTrainEvents: "0" [deprecated: Use *BaggedSampleFraction* instead: Number of randomly picked training events used in randomised (and bagged) trees]
NNodesMax: "0" [deprecated: Use MaxDepth instead to limit the tree size]
##


#VAR -*-*-*-*-*-*-*-*-*-*-*-* variables *-*-*-*-*-*-*-*-*-*-*-*-

NVar 7
max_eta                       max_eta                       max_eta                       max_eta                                                         'F'    [0.00606918334961,2.49267578125]
lep_eta1                      lep_eta1                      lep_eta1                      lep_eta1                                                        'F'    [-2.49169921875,2.49267578125]
inv_m                         inv_m                         inv_m                         inv_m                                                           'F'    [15.0092744827,1828.72692871]
dRHJ                          dRHJ                          dRHJ                          dRHJ                                                            'F'    [0.400012075901,4.19309711456]
dRLJ                          dRLJ                          dRLJ                          dRLJ                                                            'F'    [0.40001052618,4.0482134819]
dPhi0                         dPhi0                         dPhi0                         dPhi0                                                           'F'    [-3.14158391953,3.14149236679]
dPhi1                         dPhi1                         dPhi1                         dPhi1                                                           'F'    [-3.14158391953,3.14158391953]
NSpec 0


============================================================================ */

#include <array>
#include <vector>
#include <cmath>
#include <string>
#include <iostream>

#include <algorithm>
#include <limits>

#define NN new BDT2_20_GradNode

#ifndef BDT2_20_GradNode__def
#define BDT2_20_GradNode__def

class BDT2_20_GradNode {

public:

   // constructor of an essentially "empty" node floating in space
   BDT2_20_GradNode ( BDT2_20_GradNode* left,BDT2_20_GradNode* right,
                          int selector, double cutValue, bool cutType, 
                          int nodeType, double purity, double response ) :
   fLeft         ( left         ),
   fRight        ( right        ),
   fSelector     ( selector     ),
   fCutValue     ( cutValue     ),
   fCutType      ( cutType      ),
   fNodeType     ( nodeType     ),
   fPurity       ( purity       ),
   fResponse     ( response     ){
   }

   virtual ~BDT2_20_GradNode();

   // test event if it descends the tree at this node to the right
   virtual bool GoesRight( const std::vector<double>& inputValues ) const;
   BDT2_20_GradNode* GetRight( void )  {return fRight; };

   // test event if it descends the tree at this node to the left 
   virtual bool GoesLeft ( const std::vector<double>& inputValues ) const;
   BDT2_20_GradNode* GetLeft( void ) { return fLeft; };   

   // return  S/(S+B) (purity) at this node (from  training)

   double GetPurity( void ) const { return fPurity; } 
   // return the node type
   int    GetNodeType( void ) const { return fNodeType; }
   double GetResponse(void) const {return fResponse;}

private:

   BDT2_20_GradNode*   fLeft;     // pointer to the left daughter node
   BDT2_20_GradNode*   fRight;    // pointer to the right daughter node
   int                     fSelector; // index of variable used in node selection (decision tree)   
   double                  fCutValue; // cut value applied on this node to discriminate bkg against sig
   bool                    fCutType;  // true: if event variable > cutValue ==> signal , false otherwise
   int                     fNodeType; // Type of node: -1 == Bkg-leaf, 1 == Signal-leaf, 0 = internal 
   double                  fPurity;   // Purity of node from training
   double                  fResponse; // Regression response value of node
}; 

//_______________________________________________________________________
   BDT2_20_GradNode::~BDT2_20_GradNode()
{
   if (fLeft  != NULL) delete fLeft;
   if (fRight != NULL) delete fRight;
}; 

//_______________________________________________________________________
bool BDT2_20_GradNode::GoesRight( const std::vector<double>& inputValues ) const
{
   // test event if it descends the tree at this node to the right
   bool result;
     result = (inputValues[fSelector] >= fCutValue );
   if (fCutType == true) return result; //the cuts are selecting Signal ;
   else return !result;
}

//_______________________________________________________________________
bool BDT2_20_GradNode::GoesLeft( const std::vector<double>& inputValues ) const
{
   // test event if it descends the tree at this node to the left
   if (!this->GoesRight(inputValues)) return true;
   else return false;
}

#endif

#ifndef IClassifierReader__def
#define IClassifierReader__def

class IClassifierReader {

 public:

   // constructor
   IClassifierReader() : fStatusIsClean( true ) {}
   virtual ~IClassifierReader() {}

   // return classifier response
   virtual double GetMvaValue( const std::vector<double>& inputValues ) const = 0;

   // returns classifier status
   bool IsStatusClean() const { return fStatusIsClean; }

 protected:

   bool fStatusIsClean;
};

#endif

class ReadBDT2_20_Grad : public IClassifierReader {

 public:

   // constructor
   ReadBDT2_20_Grad( std::vector<std::string>& theInputVars )
      : IClassifierReader(),
        fClassName( "ReadBDT2_20_Grad" ),
        fNvars( 7 )
   {
      // the training input variables
      const char* inputVars[] = { "max_eta", "lep_eta1", "inv_m", "dRHJ", "dRLJ", "dPhi0", "dPhi1" };

      // sanity checks
      if (theInputVars.size() <= 0) {
         std::cout << "Problem in class \"" << fClassName << "\": empty input vector" << std::endl;
         fStatusIsClean = false;
      }

      if (theInputVars.size() != fNvars) {
         std::cout << "Problem in class \"" << fClassName << "\": mismatch in number of input values: "
                   << theInputVars.size() << " != " << fNvars << std::endl;
         fStatusIsClean = false;
      }

      // validate input variables
      for (size_t ivar = 0; ivar < theInputVars.size(); ivar++) {
         if (theInputVars[ivar] != inputVars[ivar]) {
            std::cout << "Problem in class \"" << fClassName << "\": mismatch in input variable names" << std::endl
                      << " for variable [" << ivar << "]: " << theInputVars[ivar].c_str() << " != " << inputVars[ivar] << std::endl;
            fStatusIsClean = false;
         }
      }

      // initialize min and max vectors (for normalisation)
      fVmin[0] = 0;
      fVmax[0] = 0;
      fVmin[1] = 0;
      fVmax[1] = 0;
      fVmin[2] = 0;
      fVmax[2] = 0;
      fVmin[3] = 0;
      fVmax[3] = 0;
      fVmin[4] = 0;
      fVmax[4] = 0;
      fVmin[5] = 0;
      fVmax[5] = 0;
      fVmin[6] = 0;
      fVmax[6] = 0;

      // initialize input variable types
      fType[0] = 'F';
      fType[1] = 'F';
      fType[2] = 'F';
      fType[3] = 'F';
      fType[4] = 'F';
      fType[5] = 'F';
      fType[6] = 'F';

      // initialize constants
      Initialize();

   }

   // destructor
   virtual ~ReadBDT2_20_Grad() {
      Clear(); // method-specific
   }

   // the classifier response
   // "inputValues" is a vector of input values in the same order as the
   // variables given to the constructor
   double GetMvaValue( const std::vector<double>& inputValues ) const override;

 private:

   // method-specific destructor
   void Clear();

   // common member variables
   const char* fClassName;

   const size_t fNvars;
   size_t GetNvar()           const { return fNvars; }
   char   GetType( int ivar ) const { return fType[ivar]; }

   // normalisation of input variables
   double fVmin[7];
   double fVmax[7];
   double NormVariable( double x, double xmin, double xmax ) const {
      // normalise to output range: [-1, 1]
      return 2*(x - xmin)/(xmax - xmin) - 1.0;
   }

   // type of input variable: 'F' or 'I'
   char   fType[7];

   // initialize internal variables
   void Initialize();
   double GetMvaValue__( const std::vector<double>& inputValues ) const;

   // private members (method specific)
   std::vector<BDT2_20_GradNode*> fForest;       // i.e. root nodes of decision trees
   std::vector<double>                fBoostWeights; // the weights applied in the individual boosts
};

double ReadBDT2_20_Grad::GetMvaValue__( const std::vector<double>& inputValues ) const
{
   double myMVA = 0;
   for (unsigned int itree=0; itree<fForest.size(); itree++){
      BDT2_20_GradNode *current = fForest[itree];
      while (current->GetNodeType() == 0) { //intermediate node
         if (current->GoesRight(inputValues)) current=(BDT2_20_GradNode*)current->GetRight();
         else current=(BDT2_20_GradNode*)current->GetLeft();
      }
      myMVA += current->GetResponse();
   }
   return 2.0/(1.0+exp(-2.0*myMVA))-1.0;
}

void ReadBDT2_20_Grad::Initialize()
{
  double inf = std::numeric_limits<double>::infinity();
  double nan = std::numeric_limits<double>::quiet_NaN();
  // itree = 0
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.584245,0.16849) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.500946,0.00189157) , 
2, 121.938, 1, 0, 0.562456,0.0624557) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.480775,-0.0384491) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.375067,-0.249865) , 
3, 1.97396, 1, 0, 0.439618,-0.0603824) , 
3, 1.62423, 1, 0, 0.542248,0.0422475) , 
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.408643,-0.182713) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.291783,-0.416435) , 
3, 1.1225, 1, 0, 0.359458,-0.140542) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.274083,-0.451834) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.131759,-0.736482) , 
3, 1.10458, 1, 0, 0.213,-0.287) , 
2, 265.888, 1, 0, 0.297558,-0.202442) , 
2, 187.744, 1, 0, 0.5,3.21229e-17)    );
  // itree = 1
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.572047,0.131714) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.532093,0.0302237) , 
4, 0.747449, 1, 0, 0.545553,0.0308807) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.449283,-0.057165) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.361833,-0.20557) , 
0, 2.18872, 1, 0, 0.425885,-0.0450812) , 
0, 1.78322, 1, 0, 0.52074,0.0151303) , 
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.478269,-0.0544123) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.391813,-0.196225) , 
5, -1.68846, 1, 0, 0.414842,-0.0752154) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.510699,0.0134862) , 
5, 1.94461, 1, 0, 0.432842,-0.0598819) , 
4, 1.44235, 1, 0, 0.5,-0.00256933)    );
  // itree = 2
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.605121,0.163549) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.523742,0.00839219) , 
5, -2.37233, 1, 0, 0.537421,0.0164535) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.62484,0.193773) , 
5, 2.24379, 1, 0, 0.552195,0.0293432) , 
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.483237,-0.0520417) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.553893,0.127053) , 
5, -0.449088, 1, 0, 0.530263,0.031313) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.462472,-0.0585163) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.5114,0.0349417) , 
6, 1.85933, 1, 0, 0.473659,-0.0173771) , 
6, -2.24399, 1, 0, 0.481549,-0.0105902) , 
3, 0.761258, 1, 0, 0.5,-0.000160355)    );
  // itree = 3
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.463054,-0.0377869) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.534701,0.0270566) , 
1, -1.14673, 1, 0, 0.52005,0.00653816) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.46083,-0.0335726) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.346895,-0.140371) , 
0, 2.13107, 1, 0, 0.432508,-0.0276793) , 
1, 1.54327, 1, 0, 0.510837,0.00293704) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.149188,-0.256504) , 
2, 360.479, 1, 0, 0.5,0.000156661)    );
  // itree = 4
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.536227,0.059696) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.451762,-0.0689521) , 
5, -2.22976, 1, 0, 0.4685,-0.0200904) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.544505,0.0926657) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.465713,-0.0755798) , 
5, 0.0497174, 1, 0, 0.521649,0.0202268) , 
6, 1.04719, 1, 0, 0.487213,-0.00589553) , 
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.572442,-0.0296307) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.575091,0.153734) , 
6, -2.1152, 1, 0, 0.574584,0.0546511) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.572208,-0.0284439) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.403102,-0.213578) , 
2, 65.9876, 1, 0, 0.473164,-0.0631867) , 
6, 0.449034, 1, 0, 0.543334,0.0183421) , 
5, 1.64552, 1, 0, 0.5,-0.000373083)    );
  // itree = 5
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.567801,0.13618) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.532645,0.0574925) , 
6, -2.88513, 1, 0, 0.548531,0.0429549) , 
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.428345,-0.103428) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.515541,0.0130237) , 
6, -0.106732, 1, 0, 0.484666,-0.0130137) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.56605,0.114212) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.47935,-0.0341968) , 
6, -0.918811, 1, 0, 0.509324,0.00771405) , 
5, 0.448745, 1, 0, 0.495007,-0.00432102) , 
6, -2.54319, 1, 0, 0.5,8.90685e-05)    );
  // itree = 6
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.592984,0.0435141) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.458676,0.145126) , 
3, 1.60632, 1, 0, 0.570036,0.0281609) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.623845,0.0317097) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.517946,-0.0466524) , 
4, 0.868263, 1, 0, 0.556408,-0.00863883) , 
2, 76.6995, 1, 0, 0.565264,0.0152742) , 
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.555204,0.0476973) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.461682,-0.0612486) , 
3, 0.549374, 1, 0, 0.470769,-0.0231288) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.443529,0.1054) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.356504,-0.0278043) , 
4, 0.747546, 1, 0, 0.376335,0.00131688) , 
0, 1.66316, 1, 0, 0.437578,-0.014537) , 
2, 101.377, 1, 0, 0.5,3.6849e-05)    );
  // itree = 7
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.538616,0.0272827) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.507295,-0.0333879) , 
5, -1.3463, 1, 0, 0.515828,-0.00766196) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.57507,-0.0384132) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.558102,0.11232) , 
5, -0.448033, 1, 0, 0.565173,0.0231043) , 
6, 2.54319, 1, 0, 0.52074,-0.00459935) , 
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.422186,0.100151) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.408194,0.00935211) , 
6, -0.149375, 1, 0, 0.414842,0.0229601) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.510699,-0.0482008) , 
5, 1.94461, 1, 0, 0.432842,0.0145865) , 
4, 1.44235, 1, 0, 0.5,-7.23092e-05)    );
  // itree = 8
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.542538,-0.0360241) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.507506,0.00485025) , 
5, -2.24386, 1, 0, 0.512296,-0.000305934) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.49276,0.11955) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.456041,0.0247506) , 
4, 1.73664, 1, 0, 0.467232,0.024267) , 
4, 1.66571, 1, 0, 0.508344,0.00184889) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.361121,-0.126821) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.376048,-0.0170414) , 
6, -0.148841, 1, 0, 0.369223,-0.0295118) , 
4, 1.96353, 1, 0, 0.5,-3.21101e-05)    );
  // itree = 9
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.487671,-0.174101) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.553902,0.0537072) , 
5, -1.64581, 1, 0, 0.544925,0.0102022) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.53538,0.020561) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.488491,-0.0316915) , 
5, -0.747891, 1, 0, 0.50695,-0.00508518) , 
6, -1.64559, 1, 0, 0.515544,-0.0016254) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.392058,-0.040358) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.412898,0.0655414) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.356728,-0.0216057) , 
4, 1.60635, 1, 0, 0.398437,0.0185847) , 
6, -1.64561, 1, 0, 0.397012,0.0105046) , 
0, 2.01904, 1, 0, 0.5,-3.46977e-05)    );
  // itree = 10
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.493419,-0.0294962) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.545139,0.081561) , 
5, -0.919355, 1, 0, 0.526236,0.0184727) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.519949,0.00393938) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.432895,-0.0435652) , 
5, -0.919007, 1, 0, 0.481093,-0.00784469) , 
6, -2.24399, 1, 0, 0.487398,-0.00416861) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.57034,-0.0706553) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.562556,0.0428193) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.311159,-0.0368976) , 
2, 175.479, 1, 0, 0.51414,0.0133114) , 
6, -2.24398, 1, 0, 0.521776,0.00728328) , 
5, 0.747939, 1, 0, 0.5,2.92681e-05)    );
  // itree = 11
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.546219,-0.00981449) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.550177,-0.0485515) , 
6, 1.04732, 1, 0, 0.547468,-0.0100705) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.546926,0.0607766) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.480723,-0.0121001) , 
4, 0.63168, 1, 0, 0.499936,0.00401312) , 
3, 1.10482, 1, 0, 0.526633,-0.0038971) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.573965,0.0728802) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.429878,-0.0210473) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.472283,0.0153054) , 
1, -0.59289, 1, 0, 0.458004,0.00142216) , 
3, 0.580635, 1, 0, 0.469384,0.0045313) , 
4, 1.09491, 1, 0, 0.5,2.38923e-05)    );
  // itree = 12
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.51809,0.0255231) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.424056,-0.0680711) , 
5, -2.14432, 1, 0, 0.457725,-0.0157752) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.528904,0.0191789) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.46196,-0.0884812) , 
6, 0.106797, 1, 0, 0.52311,0.00435059) , 
5, -0.149707, 1, 0, 0.496724,-0.00377117) , 
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.556161,-0.0430383) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.548781,0.0702599) , 
5, -2.24382, 1, 0, 0.550899,0.0173632) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.438642,-0.0348954) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.550201,0.079923) , 
6, 2.75691, 1, 0, 0.458095,-0.006829) , 
5, -0.448708, 1, 0, 0.504519,0.00527276) , 
6, 0.448798, 1, 0, 0.5,2.97526e-05)    );
  // itree = 13
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.563174,0.0485103) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.561276,-0.0108603) , 
2, 60.2488, 1, 0, 0.562029,0.00601614) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.429054,-0.0157717) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.508674,0.0662451) , 
5, 2.543, 1, 0, 0.437285,-0.00325023) , 
2, 101.377, 1, 0, 0.498266,0.00127965) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.536355,-0.0595727) , 
6, 2.84239, 1, 0, 0.5,8.36152e-06)    );
  // itree = 14
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.479744,0.0128872) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.572502,0.122019) , 
5, 0.705081, 1, 0, 0.498651,0.0159216) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.552254,0.03161) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.466905,-0.0128002) , 
5, -2.28659, 1, 0, 0.484239,-0.00170132) , 
6, -1.34639, 1, 0, 0.488106,0.00302656) , 
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.580581,-0.00136798) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.572442,-0.200872) , 
5, 2.11553, 1, 0, 0.576499,-0.0419768) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.527698,0.0258397) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.481545,-0.0993406) , 
6, 1.77391, 1, 0, 0.519692,0.00160261) , 
6, -1.64526, 1, 0, 0.532417,-0.00815895) , 
5, 1.34633, 1, 0, 0.5,2.40457e-05)    );
  // itree = 15
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.575633,0.0557209) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.541344,-0.0422744) , 
6, -2.11557, 1, 0, 0.547459,-0.0110242) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.525545,-0.0379597) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.465337,0.0107334) , 
0, 0.71903, 1, 0, 0.475834,0.000888351) , 
3, 0.76131, 1, 0, 0.494547,-0.00222399) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.532431,-0.171458) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.546879,0.123176) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.503709,0.0101871) , 
5, 1.00442, 1, 0, 0.534403,0.0410408) , 
5, -1.346, 1, 0, 0.533966,0.0150175) , 
6, 2.24399, 1, 0, 0.5,0.000161074)    );
  // itree = 16
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.583841,0.0125103) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.402902,0.0927847) , 
3, 1.95079, 1, 0, 0.572162,0.00805809) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.442899,-0.0803091) , 
1, 1.78188, 1, 0, 0.565264,0.00561357) , 
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.541138,-0.00639979) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.290628,-0.144088) , 
2, 177.502, 1, 0, 0.449223,-0.0236072) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.416866,-0.0130651) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.490193,0.0292084) , 
5, 1.64547, 1, 0, 0.435058,-0.00115422) , 
6, -1.94481, 1, 0, 0.437578,-0.0051491) , 
2, 101.377, 1, 0, 0.5,0.000112459)    );
  // itree = 17
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.562046,0.0191262) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.397328,-0.104774) , 
2, 67.4818, 1, 0, 0.471387,-0.021658) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.540945,0.103968) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.556354,-0.00694905) , 
6, 1.09025, 1, 0, 0.548365,0.0230056) , 
6, -0.448798, 1, 0, 0.523805,0.0087556) , 
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.617421,0.0753356) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.509529,-0.00126206) , 
0, 0.825293, 1, 0, 0.536509,0.00798813) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.457624,-0.0331815) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.5067,0.0122208) , 
6, 1.31793, 1, 0, 0.475969,-0.00717306) , 
6, -1.64559, 1, 0, 0.491367,-0.00331701) , 
5, -1.34642, 1, 0, 0.5,-0.000103931)    );
  // itree = 18
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.568648,-0.00637811) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.641368,0.0385213) , 
5, 1.94476, 1, 0, 0.582094,0.000780093) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.43786,-0.0347483) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.305806,-0.136967) , 
3, 0.948605, 1, 0, 0.377135,-0.0348641) , 
2, 160.412, 1, 0, 0.547248,-0.00527984) , 
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.529316,-0.0111895) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.308539,0.0551913) , 
2, 187.775, 1, 0, 0.482304,0.00029808) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.47474,0.178795) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.335011,-0.00140721) , 
0, 1.49816, 1, 0, 0.374441,0.0215422) , 
3, 1.66441, 1, 0, 0.462421,0.00421416) , 
0, 1.19017, 1, 0, 0.5,8.25492e-06)    );
  // itree = 19
  fBoostWeights.push_back(1);
  fForest.push_back( 
NN(
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.48489,0.000921106) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.516437,-0.0574391) , 
5, 1.94461, 1, 0, 0.490665,-0.00434345) , 
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.566243,-0.0155636) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.517635,0.0199815) , 
4, 0.744634, 1, 0, 0.530243,0.0049258) , 
1, -0.59289, 1, 0, 0.517497,0.00194049) , 
NN(
NN(
NN(
0, 
0, 
-1, 0, 1, -99, 0.402795,-0.0626782) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.437622,0.138681) , 
2, 101.667, 1, 0, 0.415724,0.00461971) , 
NN(
0, 
0, 
-1, 0, 1, -99, 0.171343,-0.219212) , 
2, 155.012, 1, 0, 0.340165,-0.0191826) , 
3, 1.845, 1, 0, 0.5,-0.00014363)    );
   return;
};

// Clean up
inline void ReadBDT2_20_Grad::Clear() 
{
   for (unsigned int itree=0; itree<fForest.size(); itree++) { 
      delete fForest[itree]; 
   }
}

inline double ReadBDT2_20_Grad::GetMvaValue( const std::vector<double>& inputValues ) const
{
   // classifier response value
   double retval = 0;

   // classifier response, sanity check first
   if (!IsStatusClean()) {
      std::cout << "Problem in class \"" << fClassName << "\": cannot return classifier response"
                << " because status is dirty" << std::endl;
   }
   else {
         retval = GetMvaValue__( inputValues );
   }

   return retval;
}
